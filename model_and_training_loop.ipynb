{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f71f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aab719ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install -q -U einops datasets matplotlib tqdm\n",
    "\n",
    "import math\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from torch.optim import AdamW\n",
    "from PIL import Image\n",
    "import requests\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import dnnlib\n",
    "#import tor# Removed num_to_groups\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from utils.ema_pytorch import EMA\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from cleanfid import fid\n",
    "\n",
    "#from utils.losses_samples import *\n",
    "from utils.blocks import *\n",
    "from utils.elucidating import *\n",
    "\n",
    "from collections import namedtuple\n",
    "from einops import reduce\n",
    "\n",
    "\n",
    "import cv2\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\"random seed initialization to guarantee reproducability\"\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc8f0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df20e488",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "327a1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    \"Unet Module based on the implementation of https://github.com/lucidrains/denoising-diffusion-pytorch\"\n",
    "    \"Extendedy by Skip-SE layers, additional conditional embedding for augmentation based on https://arxiv.org/abs/2206.00364\"\n",
    "    \"and patch-based diffusion https://arxiv.org/abs/2207.04316. However, we used 1 patch, as higher number of patches increased training time.\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        init_dim=None,\n",
    "        out_dim=None,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        channels=3,\n",
    "        with_time_emb=True,\n",
    "        resnet_block_groups=8,\n",
    "        use_convnext=True,\n",
    "        convnext_mult=3,\n",
    "        num_patches=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # determine dimensions, also for patch-based diffusion\n",
    "        self.channels = channels\n",
    "        self.patch_size = num_patches\n",
    "        \n",
    "        self.init_dim = channels * num_patches**2\n",
    "        init_dim2 = default(init_dim, dim // 3 * 2)\n",
    "        self.init_conv = nn.Conv2d(self.init_dim, init_dim2, 7, padding=3)\n",
    "\n",
    "        dims = [init_dim2, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "        print(\"init_dim:\\t\", init_dim)\n",
    "        print(\"dims:\\t\\t\", dims)\n",
    "        print(\"in_out:\\t\\t\", in_out)\n",
    "        \n",
    "        if use_convnext:\n",
    "            block_klass = partial(ConvNextBlock, mult=convnext_mult)\n",
    "        else:\n",
    "            block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
    "\n",
    "        # time and augmentation embeddings embeddings\n",
    "        if with_time_emb:\n",
    "            time_dim = dim * 4\n",
    "            self.time_mlp = nn.Sequential(\n",
    "                SinusoidalPositionEmbeddings(dim),\n",
    "                nn.Linear(dim, time_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(time_dim, time_dim),\n",
    "            )\n",
    "            augm_dim = dim * 4\n",
    "            self.aug_mlp = nn.Sequential(\n",
    "                nn.Linear(12, dim),\n",
    "                nn.Linear(dim, time_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(time_dim, time_dim),\n",
    "            )\n",
    "        else:\n",
    "            time_dim = None\n",
    "            self.time_mlp = None\n",
    "\n",
    "        # layers with additional SE-layers starting from the second block\n",
    "        # SE-layers are used upwards and downwards\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "            \n",
    "            if ind < 1:\n",
    "                self.downs.append(\n",
    "                    nn.ModuleList(\n",
    "                        [\n",
    "                            block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                            block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
    "                            Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                            Downsample(dim_out) if not is_last else nn.Identity(),\n",
    "                            None\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                self.downs.append(\n",
    "                    nn.ModuleList(\n",
    "                        [\n",
    "                            block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                            block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
    "                            Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                            Downsample(dim_out) if not is_last else nn.Identity(),\n",
    "                            SEBlock(in_out[ind-1][1], dim_out)\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "            \n",
    "            if ind < 1:\n",
    "                self.ups.append(\n",
    "                    nn.ModuleList(\n",
    "                        [\n",
    "                            block_klass(dim_out * 2, dim_out, time_emb_dim=time_dim),\n",
    "                            block_klass(dim_out, dim_in, time_emb_dim=time_dim),\n",
    "                            Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                            Upsample(dim_in) if not is_last else nn.Identity(),\n",
    "                            None\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                self.ups.append(\n",
    "                    nn.ModuleList(\n",
    "                        [\n",
    "                            block_klass(dim_out * 2, dim_in, time_emb_dim=time_dim),\n",
    "                            block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                            Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                            Upsample(dim_in) if not is_last else nn.Identity(),\n",
    "                            SEBlock(in_out[len(in_out)-ind][1], dim_in)\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        out_dim = default(out_dim, channels)\n",
    "        self.final_conv = nn.Sequential(\n",
    "            block_klass(dim, dim), nn.Conv2d(dim, self.init_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def convert_image_to_patches(self, x):\n",
    "        \"patch-based dfiffusion function at the beginning of the forward function\"\n",
    "        p = self.patch_size\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 3, 1) #BHWC format, bc reshape is done on last 2 axes\n",
    "        x = x.reshape(B, H, W//p, C*p) #reshape from width axis to channel axis\n",
    "        x = x.permute(0, 2, 1, 3) #now height & channel should be last 2 axes\n",
    "        x = x.reshape(B, W//p, H//p, C*p*p) #reshape from height axis to channel axis\n",
    "        return x.permute(0, 3, 2, 1) #convert to channels-first format\n",
    "    \n",
    "    def convert_patches_to_image(self, x):\n",
    "        \"patch-based difffusion fuknction at the end of the forward function\"\n",
    "        p = self.patch_size\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0,3,2,1) #BWHC; from_patches starts w/ height axis, not width\n",
    "        x.reshape(B, W, H*p, C//p) #reshape from channel axis to height axis\n",
    "        x = x.permute(0,2,1,3) #now width & channel should be last 2 axes\n",
    "        x = x.reshape(B, H*p, W*p, C//(p*p)) #reshape from channel axis to width axis\n",
    "        return x.permute(0, 3, 1, 2) #convert to channels-first format\n",
    "\n",
    "\n",
    "    def forward(self, x, time, augm):\n",
    "        x = self.convert_image_to_patches(x)\n",
    "        x = self.init_conv(x)\n",
    "        t = self.time_mlp(time) if exists(self.time_mlp) else None\n",
    "        aug = self.aug_mlp(augm)\n",
    "        h = []\n",
    "        se_up = []\n",
    "        se_down = []\n",
    "\n",
    "        # downsample\n",
    "        for block1, block2, attn, downsample, se_layer in self.downs:\n",
    "            x = block1(x, t, aug)\n",
    "            x = block2(x, t, aug)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "            se_down.append(x) \n",
    "            x = downsample(x)\n",
    "            if se_layer is not None:\n",
    "                x = se_layer(se_down.pop(0), x)\n",
    "            \n",
    "\n",
    "        # bottleneck\n",
    "        x = self.mid_block1(x, t, aug)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t, aug)\n",
    "        se_up.append(x)\n",
    "        \n",
    "        #upsample\n",
    "        for block1, block2, attn, upsample, se_layer in self.ups:\n",
    "            \n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block1(x, t, aug)\n",
    "            x = block2(x, t, aug)\n",
    "            x = attn(x)\n",
    "            se_up.append(x)\n",
    "            x = upsample(x)\n",
    "            if se_layer is not None:\n",
    "                x = se_layer(se_up.pop(0), x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        x = self.convert_patches_to_image(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42567a84",
   "metadata": {},
   "source": [
    "### Sample and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91fb39c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"The following code snippet describes the sampling functions of DDPM, implementation based on https://github.com/lucidrains/denoising-diffusion-pytorch\"\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if callable(d) else d\n",
    "\n",
    "# Forward diffusion process, based on the property of the alphas (We do not need to calculate each forward step as for backward diffusion (inference),\n",
    "# but we can utilized the mathematical property that directly allows ous to apply noise of any timestep (noise schedule step) on the image\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t.long(), x_start.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
    "    )\n",
    "\n",
    "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "\n",
    "def get_noisy_image(x_start, t):\n",
    "    # add noise\n",
    "    x_noisy = q_sample(x_start, t=t)\n",
    "\n",
    "    # turn back into PIL image\n",
    "    noisy_image = reverse_transform(x_noisy.squeeze())\n",
    "    return noisy_image\n",
    "\n",
    "\n",
    "\n",
    "#SAMPLING_IMGAES\n",
    "#----------------------------------\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample(model, x, t, t_index, batch_size):\n",
    "    betas_t = extract(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
    "    # Equation 11 in the paper\n",
    "    # Use our model (noise predictor) to predict the mean\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t, torch.zeros(batch_size, 12).to(device)) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "\n",
    "    if t_index == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
    "        noise = torch.randn_like(x)\n",
    "        # Algorithm 2 line 4:\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "\n",
    "##New\n",
    "def p_mean_variance(x, t, x_self_cond = None, clip_denoised = True):\n",
    "        preds = model_predictions(x, t, x_self_cond)\n",
    "        x_start = preds.pred_x_start\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_start.clamp_(-1., 1.)\n",
    "\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start = x_start, x_t = x, t = t)\n",
    "        return model_mean, posterior_variance, posterior_log_variance, x_start\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample_new(model, x, t: int, x_self_cond = None, augm=None):\n",
    "    b, *_, device = *x.shape, x.device\n",
    "    batched_times = torch.full((b,), t, device = x.device, dtype = torch.long)\n",
    "    model_mean, _, model_log_variance, x_start = p_mean_variance(x = x, t = batched_times, x_self_cond = x_self_cond, clip_denoised = True)\n",
    "    noise = torch.randn_like(x) if t > 0 else 0. # no noise if t == 0\n",
    "    pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n",
    "    return pred_img, x_start\n",
    "\n",
    "# Algorithm 2 (including returning all images)\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(model, shape, batch_size):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    b = shape[0]\n",
    "    # start from pure noise (for each example in the batch)\n",
    "    img = torch.randn(shape, device=device)\n",
    "    imgs = []\n",
    "\n",
    "    for i in reversed(range(0, timesteps)):\n",
    "        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i, batch_size)\n",
    "        #imgs.append(img.cpu().numpy())\n",
    "        imgs.append(img)\n",
    "    return imgs\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample_loop_new(model, shape, return_all_timesteps = False, device='cuda', augm=None, self_condition=False, num_timesteps=1000):\n",
    "    batch, device = shape[0], device\n",
    "\n",
    "    img = torch.randn(shape, device = device)\n",
    "    imgs = [img]\n",
    "\n",
    "    x_start = None\n",
    "\n",
    "    for t in tqdm(reversed(range(0, num_timesteps)), desc = 'sampling loop time step', total = num_timesteps):\n",
    "        self_cond = x_start if self_condition else None\n",
    "        img, x_start = p_sample_new(model, img, t, augm=augm)\n",
    "        imgs.append(img)\n",
    "\n",
    "    ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
    "\n",
    "    #ret = self.unnormalize(ret)\n",
    "    return ret\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, image_size, batch_size=16, channels=3):\n",
    "    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size), batch_size=batch_size)\n",
    "\n",
    "def p_losses_old(denoise_model, x_start, t, noise=None, loss_type=\"l1\", augm=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
    "    predicted_noise = denoise_model(x_noisy, t, augm)\n",
    "\n",
    "    if loss_type == 'l1':\n",
    "        loss = F.l1_loss(noise, predicted_noise)\n",
    "    elif loss_type == 'l2':\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "    elif loss_type == \"huber\":\n",
    "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def p_losses(model, x_start, t, noise = None, augm=None):\n",
    "        b, c, h, w = x_start.shape\n",
    "\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        # offset noise - https://www.crosslabs.org/blog/diffusion-with-offset-noise\n",
    "\n",
    "\n",
    "        if offset_noise_strength > 0.:\n",
    "            offset_noise = torch.randn(x_start.shape[:2], device = device)\n",
    "            noise += offset_noise_strength * rearrange(offset_noise, 'b c -> b c 1 1')\n",
    "\n",
    "        # noise sample\n",
    "\n",
    "        x = q_sample(x_start = x_start, t = t, noise = noise)\n",
    "\n",
    "        # if doing self-conditioning, 50% of the time, predict x_start from current set of times\n",
    "        # and condition with unet with that\n",
    "        # this technique will slow down training by 25%, but seems to lower FID significantly\n",
    "\n",
    "        x_self_cond = None\n",
    "        if self_condition and random() < 0.5:\n",
    "            with torch.no_grad():\n",
    "                x_self_cond = self.model_predictions(x, t).pred_x_start\n",
    "                x_self_cond.detach_()\n",
    "\n",
    "        # predict and take gradient step\n",
    "\n",
    "        model_out = model(x.half(), t.half(), augm.half())\n",
    "\n",
    "        if objective == 'pred_noise':\n",
    "            target = noise\n",
    "        elif objective == 'pred_x0':\n",
    "            target = x_start\n",
    "        elif objective == 'pred_v':\n",
    "            v = predict_v(x_start, t, noise)\n",
    "            target = v\n",
    "        else:\n",
    "            raise ValueError(f'unknown objective {objective}')\n",
    "\n",
    "        loss = F.mse_loss(model_out, target, reduction = 'none')\n",
    "        loss = reduce(loss, 'b ... -> b (...)', 'mean')\n",
    "\n",
    "        loss = loss * extract(loss_weight, t, loss.shape)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec3522ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ModelPrediction =  namedtuple('ModelPrediction', ['pred_noise', 'pred_x_start'])\n",
    "\n",
    "\n",
    "def predict_start_from_noise(x_t, t, noise):\n",
    "    return (\n",
    "        extract(sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "        extract(sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "    )\n",
    "\n",
    "def predict_noise_from_start(x_t, t, x0):\n",
    "    return (\n",
    "        (extract(sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n",
    "        extract(sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "    )\n",
    "\n",
    "def predict_v(self, x_start, t, noise):\n",
    "    return (\n",
    "        extract(sqrt_alphas_cumprod, t, x_start.shape) * noise -\n",
    "        extract(sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start\n",
    "    )\n",
    "\n",
    "def predict_start_from_v(self, x_t, t, v):\n",
    "    return (\n",
    "        extract(sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "        extract(sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n",
    "    )\n",
    "\n",
    "\n",
    "def model_predictions(model, x, t, x_self_cond=None, clip_x_start=False, rederive_pred_noise=False, objective='pred_noise'):\n",
    "    model_output = model(x.float(), t.float(), torch.zeros(x.shape[0], 12).to(device).float())\n",
    "    maybe_clip = partial(torch.clamp, min = -1., max = 1.) if clip_x_start else identity\n",
    "\n",
    "    if objective == 'pred_noise':\n",
    "        pred_noise = model_output\n",
    "        x_start = predict_start_from_noise(x, t, pred_noise)\n",
    "        x_start = maybe_clip(x_start)\n",
    "\n",
    "\n",
    "        if clip_x_start and rederive_pred_noise:\n",
    "            pred_noise = predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "    elif objective == 'pred_x0':\n",
    "        x_start = model_output\n",
    "        x_start = maybe_clip(x_start)\n",
    "        pred_noise = predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "    elif objective == 'pred_v':\n",
    "        v = model_output\n",
    "        x_start = predict_start_from_v(x, t, v)\n",
    "        x_start = maybe_clip(x_start)\n",
    "        pred_noise = predict_noise_from_start(x, t, x_start)\n",
    "        \n",
    "    return ModelPrediction(pred_noise, x_start)\n",
    "\n",
    "        \n",
    "\n",
    "def ddim_sample(model, shape, device, total_timesteps, sampling_timesteps, ddim_sampling_eta, return_all_timesteps = False, objective='pred_noise'):\n",
    "        \"Functio for faster sampling based on diffusion denoising implicit models (DDIM), https://arxiv.org/abs/2010.02502\"\n",
    "        model.eval()\n",
    "        \n",
    "        batch = shape[0]\n",
    "\n",
    "        times = torch.linspace(-1, total_timesteps - 1, steps = sampling_timesteps + 1)   # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps\n",
    "        times = list(reversed(times.int().tolist()))\n",
    "        time_pairs = list(zip(times[:-1], times[1:])) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]\n",
    "\n",
    "        img = torch.randn(shape, device = device)\n",
    "        imgs = [img]\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):\n",
    "            time_cond = torch.full((batch,), time, device = device, dtype = torch.long)\n",
    "            #self_cond = x_start if self.self_condition else None\n",
    "            pred_noise, x_start, *_ = model_predictions(model, img, time_cond, x_self_cond=None, clip_x_start = True, rederive_pred_noise = True, objective=objective)\n",
    "\n",
    "            if time_next < 0:\n",
    "                img = x_start\n",
    "                imgs.append(img)\n",
    "                continue\n",
    "\n",
    "            alpha = alphas_cumprod[time]\n",
    "            alpha_next = alphas_cumprod[time_next]\n",
    "\n",
    "            sigma = ddim_sampling_eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n",
    "            c = (1 - alpha_next - sigma ** 2).sqrt()\n",
    "\n",
    "            noise = torch.randn_like(img)\n",
    "\n",
    "            img = x_start * alpha_next.sqrt() + \\\n",
    "                  c * pred_noise + \\\n",
    "                  sigma * noise\n",
    "\n",
    "            imgs.append(img)\n",
    "\n",
    "        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
    "\n",
    "        #ret = unnormalize(ret)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8e7518",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02037a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run(dataset=\"pokemon\", batch_size=12, image_size=64, ts=1000, noise_schedule=\"sigmoid\", dim=32, channels=3, dim_mults=(1,2,4,8), aug_factor=0.12, device=\"cuda\", sample_iteration=50000, save_iteration=50000, num_samples=20,\n",
    "       optim=\"Adam\", lr=1e-4, loss_type=\"huber\", momentum=0.95, max_iter=15000, checkpoint=None, num_patches=1):\n",
    "    '''This function specifies the whole run, including the batch size, the dataset, the size of the model, the augmentation factor and so on. \n",
    "    It defines the alphas for the sampling functions based on the chosen number of timesteps.\n",
    "    Loads the data and creates a result folder, if it does not exist already.'''\n",
    "    \n",
    "    \n",
    "    #Load the data and generate the directory path\n",
    "    dataloader = get_data(r\"./train_data/\"+dataset+\"/\", batch_size, image_size)\n",
    "    #directory = os.fsencode(r\"../First_Try/\"+dataset+\"/img/\")\n",
    "    \n",
    "    #Each training run gets a unique identifier based on the chosen properties of the run\n",
    "    identifier = dataset+str(image_size)+\"_\"+noise_schedule+\"_\"+str(batch_size)+\"_\"+str(dim)+\"_\"+str(dim_mults)+\"_\"+str(ts)+\"_\"+str(aug_factor)+\"_\"+str(loss_type)+str(lr)+\"_\"+str(num_patches)\n",
    "    \n",
    "    #\n",
    "    results_top_folder = Path(\"./model_weights/\"+dataset+\"/\")\n",
    "    results_top_folder.mkdir(exist_ok = True)\n",
    "    \n",
    "    #Create folder for this run\n",
    "    results_folder = Path(\"./model_weights/\"+dataset+\"/\"+identifier+\"/\")\n",
    "    results_folder.mkdir(exist_ok = True)\n",
    "    \n",
    "    \n",
    "    #create the model\n",
    "    model = Unet(\n",
    "        dim=dim,\n",
    "        channels=channels,\n",
    "        dim_mults=dim_mults,\n",
    "        num_patches=num_patches\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    num_params = sum([p.numel() for p in model.parameters()])   \n",
    "    \n",
    "    \n",
    "    #Defining the learning rate schedule\n",
    "    if optim == \"Adam\":\n",
    "        optimizer = AdamW(model.parameters(), lr=lr, betas=(0.9,0.99))\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    #Defining the augmentation pipeline using the chosen augmentation factor\n",
    "    eluAug = AugmentPipe(aug_factor, xflip=1, yflip=1, rotate_int=1, translate_int=1, scale=1, rotate_frac=1, aniso=1, saturation=1)\n",
    "    \n",
    "    global timesteps\n",
    "    global betas\n",
    "    global alphas\n",
    "    global alphas_cumprod\n",
    "    global alphas_cumprod_prev\n",
    "    global sqrt_recip_alphas\n",
    "    global sqrt_recip_alphas_cumprod\n",
    "    global sqrt_recipm1_alphas_cumprod\n",
    "    global sqrt_alphas_cumprod\n",
    "    global sqrt_one_minus_alphas_cumprod\n",
    "    global posterior_variance\n",
    "    \n",
    "    #new\n",
    "    global snr\n",
    "    global maybe_clipped_snr\n",
    "    global loss_weight\n",
    "    global offset_noise_strength\n",
    "    global self_condition\n",
    "    global objective\n",
    "\n",
    "    #Chosing the noise schedule variant\n",
    "    #Even though it is reportat that cosine outperforms the linear schedule, we could not observe performance gains\n",
    "    #We sticked to the linear schedule\n",
    "    if noise_schedule == \"sigmoid\":\n",
    "        betas = sigmoid_beta_schedule(timesteps=ts)\n",
    "    elif noise_schedule == \"linear\":\n",
    "        betas = linear_beta_schedule(timesteps=ts)\n",
    "    elif noise_schedule == \"cosine\":\n",
    "        betas = cosine_beta_schedule(timesteps=ts)\n",
    "    elif noise_schedule == \"squared\":\n",
    "        betas = sigmoid_beta_schedule(timesteps=ts)\n",
    "    else:\n",
    "        print(\"No valid noise schedule picked\")\n",
    "        \n",
    "    timesteps = ts\n",
    "    \n",
    "    #calc_schedules(betas, ts, device)\n",
    "    # define alphas \n",
    "    alphas = 1. - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "    alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "    sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "    sqrt_recip_alphas_cumprod = torch.sqrt(1.0/alphas_cumprod)\n",
    "\n",
    "    # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "    sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "    sqrt_recipm1_alphas_cumprod =  torch.sqrt(1. / alphas_cumprod - 1)\n",
    "\n",
    "    # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "    posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "    \n",
    "    #snr_weighting\n",
    "    #https://arxiv.org/abs/2303.09556\n",
    "    snr = alphas_cumprod / (1 - alphas_cumprod)\n",
    "    maybe_clipped_snr = snr.clone()\n",
    "    min_snr_gamma = 5\n",
    "    maybe_clipped_snr.clamp_(max = min_snr_gamma)\n",
    "    \n",
    "    #offset noise\n",
    "    offset_noise_strength = 0.1\n",
    "    \n",
    "    #self-conditioning\n",
    "    self_condition = False\n",
    "    \n",
    "    #Objective\n",
    "    #We find that any other than predicting the noise leads to much worse convergence times\n",
    "    objective = 'pred_noise'\n",
    "    \n",
    "    if objective == 'pred_noise':\n",
    "        loss_weight = maybe_clipped_snr / snr\n",
    "    elif objective == 'pred_x0':\n",
    "        loss_weight = maybe_clipped_snr\n",
    "    elif objective == 'pred_v':\n",
    "        loss_weight = maybe_clipped_snr / (snr+1)\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"Run_Properties: \", identifier)\n",
    "    print(\"Number of parameters: \",num_params)\n",
    "    \n",
    "    #Generate the checkpoint, if chosen to continue training the model\n",
    "    if checkpoint is not None:\n",
    "        cp = str(results_folder)+\"/\"+checkpoint\n",
    "    else:\n",
    "        cp = None\n",
    "    \n",
    "    #Call the train function\n",
    "    train(dataset, max_iter=max_iter, batch_size=batch_size, image_size=image_size, model=model, dataloader=dataloader, optimizer=optimizer, device=device, augPlan=eluAug, folders=[results_folder], save_iteration=save_iteration,\n",
    "          sample_iteration=sample_iteration, num_samples=num_samples, loss_type=loss_type, checkpoint=cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0a2b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, max_iter=150000, batch_size=12, image_size=64, model=None, dataloader=None, checkpoint=None, optimizer=None, device='cuda', augPlan=None, folders=None, loss_type=\"huber\",\n",
    "         sample_iteration=500000, save_iteration=50000, num_samples=50):\n",
    "    '''Final training loop function that is called by the run function. '''\n",
    "\n",
    "    \n",
    "    iteration = 0\n",
    "    losses = list()\n",
    "    duration = list()\n",
    "    ep = list()\n",
    "    full_tens = list()\n",
    "    iterations = list() \n",
    "    kid_mu = list()\n",
    "    kid_sigma = list()\n",
    "    #fid = list()\n",
    "    batch_lpips = list()\n",
    "    avg_single_lpips = list()\n",
    "    min_single_lpips = list()\n",
    "    \n",
    "    loss_dict = {'epoch': [],\n",
    "                'iteration': [],\n",
    "                'loss': [],\n",
    "                'sliding_loss': []\n",
    "                }\n",
    "    score_dict = {'epoch': [],\n",
    "                'iteration': [],\n",
    "                'min_lpips': [],\n",
    "                'avg_lpips': [],\n",
    "                'fid': [],\n",
    "                'kid': [],\n",
    "                }\n",
    "    \n",
    "    #Directly load the images into memory (was a little faster due to small size of dataset)\n",
    "    full_tens = list()\n",
    "    for batch in dataloader:\n",
    "        batch = batch[0].to(device)\n",
    "        full_tens.append(batch)\n",
    "    \n",
    "    \n",
    "    print(\"FULLTENS: \", torch.cat(full_tens,dim=0).shape)\n",
    "    full_tens = torch.cat(full_tens,dim=0)\n",
    "    print(\"RANDTENS: \", full_tens[torch.randint(0, full_tens.size(0), (16,))].shape)\n",
    "    \n",
    "    starting_epoch=0\n",
    "    kid_mean = -1\n",
    "    kid_std = -1\n",
    "    lpips_val = -1\n",
    "    avg_lpips = -1\n",
    "    min_lpips = -1\n",
    "    \n",
    "    #Mixed precision\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    #Load checkpoint, if wanted\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint))\n",
    "        iteration = int(checkpoint.split(\"-\")[-1][:-4])+1\n",
    "        \n",
    "        cp_optim = torch.load(checkpoint.replace('model','optimizer'))\n",
    "        optimizer.load_state_dict(torch.load(checkpoint.replace('model','optimizer')))\n",
    "    \n",
    "    #Exponentially moving average\n",
    "    ema = EMA(\n",
    "        model,\n",
    "        beta = 0.999,                 # exponential moving average factor\n",
    "        update_after_step = 20000,    # only after this number of .update() calls will it start updating\n",
    "        update_every = 10,            # how often to actually update, to save on compute (updates every 10th .update() call)\n",
    "    )\n",
    "    if checkpoint is not None:\n",
    "        ema.load_state_dict(torch.load(checkpoint.replace('model','ema'),map_location=torch.device('cuda')))\n",
    "\n",
    "    sliding_loss = []\n",
    "    for epoch in range(starting_epoch, 10000000):\n",
    "        model.train()\n",
    "        if iteration > max_iter:\n",
    "            break\n",
    "            \n",
    "        #10 iterations per epoch, however, epochs can be neglected, as dataset size is so small\n",
    "        pbar = tqdm(range(10))\n",
    "        total_loss = 0\n",
    "\n",
    "        for step in pbar:\n",
    "            batch = full_tens[torch.randint(0, full_tens.size(0), (batch_size,))]\n",
    "            iteration += 1\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #augment the images based on the chosen augmentation plan\n",
    "            augm, labels = augPlan(batch)\n",
    "\n",
    "            # Algorithm 1 line 3: sample t uniformally for every example in the batch\n",
    "            t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "            \n",
    "            #Autocast and scaler for mixed-precision training\n",
    "            with autocast():\n",
    "                loss = p_losses(model, augm, t.float(), augm=labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            ema.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({\"Loss\":total_loss/(step+1),\"epoch\":epoch,\"iteration\":iteration})\n",
    "            \n",
    "            #Sliding loss contains the loss of the last 50 iteration to account for outliers and give a more accurate loss representation\n",
    "            if len(sliding_loss) < 50:\n",
    "                sliding_loss.insert(0, loss.item())\n",
    "            else:\n",
    "                sliding_loss.insert(0, loss.item())\n",
    "                sliding_loss.pop()\n",
    "                \n",
    "                \n",
    "            #Save the loss dictionary as csv file\n",
    "            if iteration % 1000 == 1 or iteration == max_iter:\n",
    "                loss_dict['epoch'].append(epoch)\n",
    "                loss_dict['iteration'].append(iteration)\n",
    "                loss_dict['loss'].append(loss.item())\n",
    "                loss_dict['sliding_loss'].append(sum(sliding_loss)/len(sliding_loss))\n",
    "                result_df = pd.DataFrame(loss_dict)\n",
    "                result_df.to_csv(str(folders[0] / f'loss_scores.csv'))  \n",
    "\n",
    "            # Sample images and save\n",
    "            if iteration % sample_iteration == 0:\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    sample_image_ema = ddim_sample(ema, (4,3,image_size,image_size), device='cuda', total_timesteps=1000, sampling_timesteps=50, ddim_sampling_eta=0, objective=objective)\n",
    "                    sample_image_model = ddim_sample(model, (4,3,image_size,image_size), device='cuda', total_timesteps=1000, sampling_timesteps=50, ddim_sampling_eta=0, objective=objective)\n",
    "\n",
    "                print(\"\\nSAMPLED_IMAGES\")\n",
    "                sample_image_ema = map_interval(sample_image_ema)\n",
    "                sample_image_model = map_interval(sample_image_model)\n",
    "                plot_images(sample_image_ema)\n",
    "                plot_images(sample_image_model)\n",
    "\n",
    "\n",
    "                #SAVE IMAGES\n",
    "                save_image((sample_image_ema+1)*0.5, str(folders[0] / f'final_ema-{iteration}.png'), nrow = 15)\n",
    "                save_image((sample_image_model+1)*0.5, str(folders[0] / f'final_model-{iteration}.png'), nrow = 15)\n",
    "\n",
    "            #save checpoints\n",
    "            #if (iteration % save_iteration == 1) or (iteration == max_iter):\n",
    "            #    torch.save(model.state_dict(), str(folders[0]/ f'model-{iteration}.tar'))\n",
    "            #    torch.save(ema.state_dict(), str(folders[0]/ f'ema-{iteration}.tar'))\n",
    "            #    torch.save(optimizer.state_dict(), str(folders[0]/ f'optimizer-{iteration}.tar'))\n",
    "                \n",
    "            return None\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b825d2be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_dim:\t None\n",
      "dims:\t\t [42, 64, 128]\n",
      "in_out:\t\t [(42, 64), (64, 128)]\n",
      "Run_Properties:  LSUNBed96_linear_16_64_(1, 2)_1000_0.25_l20.0003_1\n",
      "Number of parameters:  6879527\n",
      "FULLTENS:  torch.Size([100, 3, 96, 96])\n",
      "RANDTENS:  torch.Size([16, 3, 96, 96])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ef1cafa8464d1fb3f514f7a7cabe70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_dim:\t None\n",
      "dims:\t\t [42, 64, 128, 128]\n",
      "in_out:\t\t [(42, 64), (64, 128), (128, 128)]\n",
      "Run_Properties:  LSUNBed96_linear_16_64_(1, 2, 2)_1000_0.5_l20.0003_1\n",
      "Number of parameters:  11434727\n",
      "FULLTENS:  torch.Size([100, 3, 96, 96])\n",
      "RANDTENS:  torch.Size([16, 3, 96, 96])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5fbab9a26c745018b48f3e3683abb07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_dim:\t None\n",
      "dims:\t\t [42, 64, 128, 256]\n",
      "in_out:\t\t [(42, 64), (64, 128), (128, 256)]\n",
      "Run_Properties:  LSUNBed96_linear_16_64_(1, 2, 4)_1000_0.5_l20.0003_1\n",
      "Number of parameters:  26803303\n",
      "FULLTENS:  torch.Size([100, 3, 96, 96])\n",
      "RANDTENS:  torch.Size([16, 3, 96, 96])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56f35d9c6684e5ebed4cb7f3ff80271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Found no valid file for the classes Obama96_linear_16_64_(1, 2)_1000_0.25_l20.0003_1. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./test_data/\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;28mlen\u001b[39m(l1)):\n\u001b[1;32m----> 6\u001b[0m         \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_mults\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ml2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m650000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_schedule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [7], line 9\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(dataset, batch_size, image_size, ts, noise_schedule, dim, channels, dim_mults, aug_factor, device, sample_iteration, save_iteration, num_samples, optim, lr, loss_type, momentum, max_iter, checkpoint, num_patches)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m'''This function specifies the whole run, including the batch size, the dataset, the size of the model, the augmentation factor and so on. \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mIt defines the alphas for the sampling functions based on the chosen number of timesteps.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mLoads the data and creates a result folder, if it does not exist already.'''\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#Load the data and generate the directory path\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./train_data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#directory = os.fsencode(r\"../First_Try/\"+dataset+\"/img/\")\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#Each training run gets a unique identifier based on the chosen properties of the run\u001b[39;00m\n\u001b[0;32m     13\u001b[0m identifier \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(image_size)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mnoise_schedule\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(batch_size)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(dim)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(dim_mults)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(ts)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(aug_factor)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(loss_type)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(lr)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(num_patches)\n",
      "File \u001b[1;32m~\\Master Thesis\\github\\utils\\losses_samples.py:331\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(path, batch_size, image_size)\u001b[0m\n\u001b[0;32m    329\u001b[0m g \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mGenerator()\n\u001b[0;32m    330\u001b[0m g\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m--> 331\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, worker_init_fn\u001b[38;5;241m=\u001b[39mseed_worker, generator\u001b[38;5;241m=\u001b[39mg,drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataloader\n",
      "File \u001b[1;32mD:\\Python\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py:310\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    304\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m ):\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32mD:\\Python\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py:146\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m    145\u001b[0m classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_classes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n\u001b[1;32m--> 146\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_to_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextensions \u001b[38;5;241m=\u001b[39m extensions\n",
      "File \u001b[1;32mD:\\Python\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py:190\u001b[0m, in \u001b[0;36mDatasetFolder.make_dataset\u001b[1;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_to_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;66;03m# prevent potential bug since make_dataset() would use the class_to_idx logic of the\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;66;03m# find_classes() function, instead of using that of the find_classes() method, which\u001b[39;00m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;66;03m# is potentially overridden and thus could have a different logic.\u001b[39;00m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe class_to_idx parameter cannot be None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_to_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Python\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py:103\u001b[0m, in \u001b[0;36mmake_dataset\u001b[1;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m         msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupported extensions are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextensions \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(extensions, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m instances\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Found no valid file for the classes Obama96_linear_16_64_(1, 2)_1000_0.25_l20.0003_1. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp"
     ]
    }
   ],
   "source": [
    "#Call the run function and train the model\n",
    "l1 = [(1,2),(1,2,2),(1,2,4)]\n",
    "l2 = [0.25, 0.5, 0.5]\n",
    "for dataset in os.listdir('./test_data/'):\n",
    "    for i in range (len(l1)):\n",
    "        run(dataset=dataset, dim=64, dim_mults=l1[i], loss_type='l2', num_samples=20, max_iter=650000, sample_iteration=10000, save_iteration=10000, aug_factor=l2[i], batch_size=16, image_size=96,\n",
    "                checkpoint=None, lr=3e-4, noise_schedule='linear', ts=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174bc19a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e6d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterthesis_kernel",
   "language": "python",
   "name": "masterthesis_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
