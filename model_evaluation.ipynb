{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a71886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eccf2fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Removed num_to_groups\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dce5c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q -U einops datasets matplotlib tqdm\n",
    "\n",
    "import math\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from torch.optim import AdamW\n",
    "from PIL import Image\n",
    "import requests\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import dnnlib\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from utils.ema_pytorch import EMA\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from cleanfid import fid\n",
    "\n",
    "from utils.losses_samples import *\n",
    "from utils.blocks import *\n",
    "from utils.elucidating import *\n",
    "from utils.persistence import *\n",
    "from utils.misc import *\n",
    "\n",
    "\n",
    "import cv2\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ab0b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83a8d122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: D:\\Python\\anaconda3\\lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    }
   ],
   "source": [
    "import lpips\n",
    "loss_fn_alex = lpips.LPIPS(net='alex').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bcdbde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        init_dim=None,\n",
    "        out_dim=None,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        channels=3,\n",
    "        with_time_emb=True,\n",
    "        resnet_block_groups=8,\n",
    "        use_convnext=True,\n",
    "        convnext_mult=2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # determine dimensions\n",
    "        self.channels = channels\n",
    "\n",
    "        init_dim = default(init_dim, dim // 3 * 2)\n",
    "        self.init_conv = nn.Conv2d(channels, init_dim, 7, padding=3)\n",
    "\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "        print(\"init_dim:\\t\", init_dim)\n",
    "        print(\"dims:\\t\\t\", dims)\n",
    "        print(\"in_out:\\t\\t\", in_out)\n",
    "        \n",
    "        if use_convnext:\n",
    "            block_klass = partial(ConvNextBlock, mult=convnext_mult)\n",
    "        else:\n",
    "            block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
    "\n",
    "        # time embeddings\n",
    "        if with_time_emb:\n",
    "            time_dim = dim * 4\n",
    "            self.time_mlp = nn.Sequential(\n",
    "                SinusoidalPositionEmbeddings(dim),\n",
    "                nn.Linear(dim, time_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(time_dim, time_dim),\n",
    "            )\n",
    "            augm_dim = dim * 4\n",
    "            self.aug_mlp = nn.Sequential(\n",
    "                nn.Linear(12, dim),\n",
    "                nn.Linear(dim, time_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(time_dim, time_dim),\n",
    "            )\n",
    "        else:\n",
    "            time_dim = None\n",
    "            self.time_mlp = None\n",
    "\n",
    "        # layers\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "            \n",
    "            if ind < 1:\n",
    "                self.downs.append(\n",
    "                    nn.ModuleList(\n",
    "                        [\n",
    "                            block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                            block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
    "                            Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                            Downsample(dim_out) if not is_last else nn.Identity(),\n",
    "                            None\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                self.downs.append(\n",
    "                    nn.ModuleList(\n",
    "                        [\n",
    "                            block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                            block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
    "                            Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                            Downsample(dim_out) if not is_last else nn.Identity(),\n",
    "                            SEBlock(in_out[ind-1][1], dim_out)\n",
    "                            #None\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "            \n",
    "            if ind < 1:\n",
    "                self.ups.append(\n",
    "                    nn.ModuleList(\n",
    "                        [\n",
    "                            block_klass(dim_out * 2, dim_out, time_emb_dim=time_dim),\n",
    "                            block_klass(dim_out, dim_in, time_emb_dim=time_dim),\n",
    "                            Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                            Upsample(dim_in) if not is_last else nn.Identity(),\n",
    "                            None\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                self.ups.append(\n",
    "                    nn.ModuleList(\n",
    "                        [\n",
    "                            block_klass(dim_out * 2, dim_in, time_emb_dim=time_dim),\n",
    "                            block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                            Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                            Upsample(dim_in) if not is_last else nn.Identity(),\n",
    "                            SEBlock(in_out[len(in_out)-ind][1], dim_in)\n",
    "                            #None\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        out_dim = default(out_dim, channels)\n",
    "        self.final_conv = nn.Sequential(\n",
    "            block_klass(dim, dim), nn.Conv2d(dim, out_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, time, augm):\n",
    "        x = self.init_conv(x)\n",
    "        t = self.time_mlp(time) if exists(self.time_mlp) else None\n",
    "        aug = self.aug_mlp(augm)\n",
    "        h = []\n",
    "        se_up = []\n",
    "        se_down = []\n",
    "\n",
    "        # downsample\n",
    "        for block1, block2, attn, downsample, se_layer in self.downs:\n",
    "            x = block1(x, t, aug)\n",
    "            x = block2(x, t, aug)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "            se_down.append(x) \n",
    "            x = downsample(x)\n",
    "            if se_layer is not None:\n",
    "                x = se_layer(se_down.pop(0), x)\n",
    "            \n",
    "\n",
    "        # bottleneck\n",
    "        x = self.mid_block1(x, t, aug)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t, aug)\n",
    "        se_up.append(x)\n",
    "        \n",
    "        #upsample\n",
    "        for block1, block2, attn, upsample, se_layer in self.ups:\n",
    "            \n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block1(x, t, aug)\n",
    "            x = block2(x, t, aug)\n",
    "            x = attn(x)\n",
    "            se_up.append(x)\n",
    "            x = upsample(x)\n",
    "            if se_layer is not None:\n",
    "                x = se_layer(se_up.pop(0), x)\n",
    "        \n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5525019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward diffusion (using the nice property)\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
    "    )\n",
    "\n",
    "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "def get_noisy_image(x_start, t):\n",
    "    # add noise\n",
    "    x_noisy = q_sample(x_start, t=t)\n",
    "\n",
    "    # turn back into PIL image\n",
    "    noisy_image = reverse_transform(x_noisy.squeeze())\n",
    "    return noisy_image\n",
    "\n",
    "\n",
    "\n",
    "#SAMPLING_IMGAES\n",
    "#----------------------------------\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample(model, x, t, t_index, batch_size):\n",
    "    betas_t = extract(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
    "    # Equation 11 in the paper\n",
    "    # Use our model (noise predictor) to predict the mean\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t, torch.zeros(batch_size, 12).to(device)) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "\n",
    "    if t_index == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
    "        noise = torch.randn_like(x)\n",
    "        # Algorithm 2 line 4:\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "\n",
    "##New\n",
    "def p_mean_variance(self, x, t, x_self_cond = None, clip_denoised = True):\n",
    "        preds = self.model_predictions(x, t, x_self_cond)\n",
    "        x_start = preds.pred_x_start\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_start.clamp_(-1., 1.)\n",
    "\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start = x_start, x_t = x, t = t)\n",
    "        return model_mean, posterior_variance, posterior_log_variance, x_start\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample_new(self, x, t: int, x_self_cond = None):\n",
    "    b, *_, device = *x.shape, x.device\n",
    "    batched_times = torch.full((b,), t, device = x.device, dtype = torch.long)\n",
    "    model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = batched_times, x_self_cond = x_self_cond, clip_denoised = True)\n",
    "    noise = torch.randn_like(x) if t > 0 else 0. # no noise if t == 0\n",
    "    pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n",
    "    return pred_img, x_start\n",
    "\n",
    "# Algorithm 2 (including returning all images)\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(model, shape, batch_size):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    b = shape[0]\n",
    "    # start from pure noise (for each example in the batch)\n",
    "    img = torch.randn(shape, device=device)\n",
    "    imgs = []\n",
    "\n",
    "    for i in reversed(range(0, timesteps)):\n",
    "        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i, batch_size)\n",
    "        #imgs.append(img.cpu().numpy())\n",
    "        imgs.append(img)\n",
    "    return imgs\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample_loop_new(self, shape, return_all_timesteps = False):\n",
    "    batch, device = shape[0], self.betas.device\n",
    "\n",
    "    img = torch.randn(shape, device = device)\n",
    "    imgs = [img]\n",
    "\n",
    "    x_start = None\n",
    "\n",
    "    for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):\n",
    "        self_cond = x_start if self.self_condition else None\n",
    "        img, x_start = self.p_sample(img, t, self_cond)\n",
    "        imgs.append(img)\n",
    "\n",
    "    ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
    "\n",
    "    ret = self.unnormalize(ret)\n",
    "    return ret\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, image_size, batch_size=16, channels=3):\n",
    "    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size), batch_size=batch_size)\n",
    "\n",
    "def p_losses(denoise_model, x_start, t, noise=None, loss_type=\"l1\", augm=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
    "    predicted_noise = denoise_model(x_noisy, t, augm)\n",
    "\n",
    "    if loss_type == 'l1':\n",
    "        loss = F.l1_loss(noise, predicted_noise)\n",
    "    elif loss_type == 'l2':\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "    elif loss_type == \"huber\":\n",
    "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82fdef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "ModelPrediction =  namedtuple('ModelPrediction', ['pred_noise', 'pred_x_start'])\n",
    "\n",
    "\n",
    "def predict_start_from_noise(x_t, t, noise):\n",
    "    return (\n",
    "        extract(sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "        extract(sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "    )\n",
    "\n",
    "def predict_noise_from_start(x_t, t, x0):\n",
    "    return (\n",
    "        (extract(sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n",
    "        extract(sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "    )\n",
    "\n",
    "def predict_v(self, x_start, t, noise):\n",
    "    return (\n",
    "        extract(sqrt_alphas_cumprod, t, x_start.shape) * noise -\n",
    "        extract(sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start\n",
    "    )\n",
    "\n",
    "def predict_start_from_v(self, x_t, t, v):\n",
    "    return (\n",
    "        extract(sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "        extract(sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n",
    "    )\n",
    "\n",
    "\n",
    "def model_predictions(model, x, t, x_self_cond=None, clip_x_start=False, rederive_pred_noise=False, objective='pred_noise'):\n",
    "    model_output = model(x, t, torch.zeros(x.shape[0], 12).to(device))\n",
    "    maybe_clip = partial(torch.clamp, min = -1., max = 1.) if clip_x_start else identity\n",
    "    \n",
    "    if objective == 'pred_noise':\n",
    "        pred_noise = model_output\n",
    "        x_start = predict_start_from_noise(x, t, pred_noise)\n",
    "        x_start = maybe_clip(x_start)\n",
    "\n",
    "        if clip_x_start and rederive_pred_noise:\n",
    "            pred_noise = predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "    elif self.objective == 'pred_x0':\n",
    "        x_start = model_output\n",
    "        x_start = maybe_clip(x_start)\n",
    "        pred_noise = predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "    elif self.objective == 'pred_v':\n",
    "        v = model_output\n",
    "        x_start = predict_start_from_v(x, t, v)\n",
    "        x_start = maybe_clip(x_start)\n",
    "        pred_noise = predict_noise_from_start(x, t, x_start)\n",
    "        \n",
    "    return ModelPrediction(pred_noise, x_start)\n",
    "\n",
    "        \n",
    "\n",
    "def ddim_sample(model, shape, device, total_timesteps, sampling_timesteps, ddim_sampling_eta, return_all_timesteps = False, objective='pred_noise'):\n",
    "        \n",
    "        batch = shape[0]\n",
    "\n",
    "        times = torch.linspace(-1, total_timesteps - 1, steps = sampling_timesteps + 1)   # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps\n",
    "        times = list(reversed(times.int().tolist()))\n",
    "        time_pairs = list(zip(times[:-1], times[1:])) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]\n",
    "\n",
    "        img = torch.randn(shape, device = device)\n",
    "        imgs = [img]\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):\n",
    "            time_cond = torch.full((batch,), time, device = device, dtype = torch.long)\n",
    "            #self_cond = x_start if self.self_condition else None\n",
    "            pred_noise, x_start, *_ = model_predictions(model, img, time_cond, x_self_cond=None, clip_x_start = True, rederive_pred_noise = True, objective=objective)\n",
    "\n",
    "            if time_next < 0:\n",
    "                img = x_start\n",
    "                imgs.append(img)\n",
    "                continue\n",
    "\n",
    "            alpha = alphas_cumprod[time]\n",
    "            alpha_next = alphas_cumprod[time_next]\n",
    "\n",
    "            sigma = ddim_sampling_eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n",
    "            c = (1 - alpha_next - sigma ** 2).sqrt()\n",
    "\n",
    "            noise = torch.randn_like(img)\n",
    "\n",
    "            img = x_start * alpha_next.sqrt() + \\\n",
    "                  c * pred_noise + \\\n",
    "                  sigma * noise\n",
    "\n",
    "            imgs.append(img)\n",
    "\n",
    "        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
    "\n",
    "        #ret = unnormalize(ret)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "85652b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_name(dataset, path, run):\n",
    "    name = dataset+\"_\"\n",
    "    dim_mults = None\n",
    "    aug_fact = None\n",
    "    conv_mult = None\n",
    "    \n",
    "    if '(1, 2, 2)' in run:\n",
    "        name += '(1,2,2)'\n",
    "        dim_mults = (1,2,2)\n",
    "        conv_mult = 2\n",
    "    elif '(1, 2)' in run:\n",
    "        name += '(1,2)'\n",
    "        dim_mults = (1,2)\n",
    "        conv_mult = 2\n",
    "    elif '(1, 2, 4)' in run:\n",
    "        name += '(1,2,4)'\n",
    "        dim_mults = (1,2,4)\n",
    "        conv_mult = 3\n",
    "    else: \n",
    "        print(\"run modelSize could not be determined: \",run)\n",
    "        return None\n",
    "    \n",
    "    name += '_'\n",
    "    \n",
    "    if '0.5' in run:\n",
    "        name += '0.5'\n",
    "        aug_fact = 0.5\n",
    "    elif '0.25' in run:\n",
    "        name += '0.25'\n",
    "        aug_fact = 0.25\n",
    "    else:\n",
    "        print(\"run augFactor could not be determined: \",run)\n",
    "        return None\n",
    "    \n",
    "    return name, dim_mults, aug_fact, conv_mult\n",
    "\n",
    "def initialize_testing():\n",
    "    global timesteps\n",
    "    global betas\n",
    "    global alphas\n",
    "    global alphas_cumprod\n",
    "    global alphas_cumprod_prev\n",
    "    global sqrt_recip_alphas\n",
    "    global sqrt_recip_alphas_cumprod\n",
    "    global sqrt_recipm1_alphas_cumprod\n",
    "    global sqrt_alphas_cumprod\n",
    "    global sqrt_one_minus_alphas_cumprod\n",
    "    global posterior_variance \n",
    "\n",
    "    betas = linear_beta_schedule(timesteps=1000)\n",
    "        \n",
    "    timesteps = 1000\n",
    "    \n",
    "    #calc_schedules(betas, ts, device)\n",
    "    # define alphas \n",
    "    alphas = 1. - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "    alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "    sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "    sqrt_recip_alphas_cumprod = torch.sqrt(1.0/alphas_cumprod)\n",
    "\n",
    "    # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "    sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "    sqrt_recipm1_alphas_cumprod =  torch.sqrt(1. / alphas_cumprod - 1)\n",
    "\n",
    "    # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "    posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "    \n",
    "    \n",
    "def calculate_lpips(real, sampled, intrinsic):\n",
    "    lpips_min_list = []\n",
    "    closest_original_list, lpips_list = [], []\n",
    "    overfit = False\n",
    "        \n",
    "    for i, sample_image in enumerate(sampled):\n",
    "        min_original_image = None\n",
    "        \n",
    "        lpips_scores = loss_fn_alex(sample_image.to(device), real.to(device))\n",
    "        lpips_min_list.append(torch.min(lpips_scores).item())\n",
    "        min_original_image = real[torch.argmin(lpips_scores)].unsqueeze(0).cpu()\n",
    "\n",
    "        closest_original_list.append(min_original_image)\n",
    "        \n",
    "    lpips_min_avg = np.mean(lpips_min_list)\n",
    "    lpips_min_min = np.mean(np.sort(lpips_min_list)[:10])\n",
    "    lpips_fraction = lpips_min_avg/intrinsic[0]\n",
    "    \n",
    "    if lpips_min_min < 0.05:\n",
    "        overfit = True\n",
    "    \n",
    "    closest_tensor = torch.stack(closest_original_list, dim=0).view(-1,3,96,96)\n",
    "    \n",
    "    return lpips_min_avg, lpips_min_min, lpips_fraction, overfit, closest_tensor\n",
    "\n",
    "def calculate_clip_FID(dataset):\n",
    "    if dataset == 'Grumpy':\n",
    "        res= !python fcd.py --path_source \"./train_data/Grumpy/img/\" --path_test \"./score_calculations/\"\n",
    "    elif dataset == 'Obama':\n",
    "        res= !python fcd.py --path_source \"./test_data/Obama/img/\" --path_test \"./score_calculations/\"\n",
    "    elif dataset == 'Cat':\n",
    "        res= !python fcd.py --path_source \"./train_data/Cat/img/\" --path_test \"./score_calculations/\"\n",
    "    elif dataset == 'Dog':\n",
    "        res= !python fcd.py --path_source \"./train_data/Dog/img/\" --path_test \"./score_calculations/\"\n",
    "    elif dataset == 'Panda':\n",
    "        res= !python fcd.py --path_source \"./train_data/Panda/img/\" --path_test \"./score_calculations/\"\n",
    "    elif dataset == 'Influ':\n",
    "        res= !python fcd.py --path_source \"./train_data/Influ/img/\" --path_test \"./score_calculations/\"\n",
    "    elif dataset == 'Poke':\n",
    "        res= !python fcd.py --path_source \"./train_data/Poke/img/\" --path_test \"./score_calculations/\"\n",
    "    elif dataset == 'Art':\n",
    "        res= !python fcd.py --path_source \"./train_data/Art/img/\" --path_test \"./score_calculations/\"\n",
    "    elif dataset == 'Fauvism':\n",
    "        res= !python fcd.py --path_source \"./train_data/Fauvism/img/\" --path_test \"./score_calculations/\"\n",
    "    elif dataset == 'Moon':\n",
    "        res= !python fcd.py --path_source \"./train_data/Moon/img/\" --path_test \"./score_calculations/\"\n",
    "    elif dataset == 'Anime':\n",
    "        res= !python fcd.py --path_source \"./train_data/Anime/img/\" --path_test \"./score_calculations/\"\n",
    "    elif dataset == 'Shells':\n",
    "        res= !python fcd.py --path_source \"./train_data/Shells/img/\" --path_test \"./score_calculations/\"\n",
    "    elif dataset == 'Skulls':\n",
    "        res= !python fcd.py --path_source \"./train_data/Skulls/img/\" --path_test \"./score_calculations/\"\n",
    "    elif dataset == 'FFHQ':\n",
    "        res= !python fcd.py --path_source \"./train_data/FFHQ/img/\" --path_test \"./score_calculations/\"\n",
    "    elif dataset == 'CelebA':\n",
    "        print(dataset)\n",
    "        res= !python fcd.py --path_source \"./train_data/CelebA/img/\" --path_test \"./score_calculations/\"\n",
    "    elif dataset == 'LSUNBed':\n",
    "        res= !python fcd.py --path_source \"./test_data/LSUNBed/img/\" --path_test \"./score_calculations/\"\n",
    "    elif dataset == 'Dining':\n",
    "        res= !python fcd.py --path_source \"./train_data/Dining/\" --path_test \"./score_calculations/\"\n",
    "    elif dataset == 'Garbage':\n",
    "        res= !python fcd.py --path_source \"./train_data/Garbage/img/\" --path_test \"./score_calculations/\"\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return res\n",
    "                                  \n",
    "\n",
    "\n",
    "def test_run_final():\n",
    "    result_dict = dict()\n",
    "    result_dict['Features'] = ['Dataset','Dim_mults','Augmentation','Iteration','FID','KID','Clip_FID','LPIPS_MinAvg','LPIPS_min','LPIPS_norm','Overfit']\n",
    "    \n",
    "    \n",
    "    model_path = './model_weights/'\n",
    "    sample_path = './score_calculations/'\n",
    "    \n",
    "    intrinsic_df = pd.read_csv('./intrinsic_scores.csv',delimiter=\";\")\n",
    "    intrinsic_df.rename(columns={'Unnamed: 0':'Name'}, inplace=True)\n",
    "    intrinsic_df.loc[:,'Name'][intrinsic_df.Name=='Shell'] = 'Shells'\n",
    "    intrinsic_df.loc[:,'Name'][intrinsic_df.Name=='Skull'] = 'Skulls'\n",
    "    \n",
    "    intrinsic_df = intrinsic_df[['Name','LPIPS']]\n",
    "    \n",
    "    display(intrinsic_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Initialize global variables\n",
    "    initialize_testing()\n",
    "    \n",
    "    #Iterate through datasets\n",
    "    dataset_list = os.listdir('./model_weights/')\n",
    "    dataset_test_list = os.listdir()\n",
    "\n",
    "    #Iterate through dataset\n",
    "    for dataset in dataset_list:\n",
    "        if dataset == '.ipynb_checkpoints':\n",
    "            continue\n",
    "            \n",
    "        dataset_path = r\"./train_data/\"+dataset\n",
    "        sample_path = './score_calculations/'\n",
    "        save_path = Path(\"./sampled_images/{}/\".format(dataset))\n",
    "        save_path.mkdir(exist_ok = True)\n",
    "        \n",
    "            \n",
    "        #load datapoints of this dataset into memory\n",
    "        dataloader = get_data(dataset_path, batch_size=44, image_size=96)\n",
    "        full_tens = list()        \n",
    "        for batch in dataloader:\n",
    "            batch = batch[0].to(device)\n",
    "            full_tens.append(batch)\n",
    "        full_tens = torch.cat(full_tens,dim=0)\n",
    "        print(full_tens.shape)\n",
    "        \n",
    "        \n",
    "               \n",
    "        for run in os.listdir(model_path+dataset+'/'):\n",
    "            print(\"run: \", run)\n",
    "            done_flag = False\n",
    "            if run == '.ipynb_checkpoints':\n",
    "                continue\n",
    "            \n",
    "            run_name, dim_mults, aug_factor, conv_mult = get_run_name(dataset=dataset, path='./model_weights/'+dataset+'/', run=run)\n",
    "            complete_path = model_path + dataset +'/'+run+'/'\n",
    "            if dim_mults == (1, 2, 2) and aug_factor == 0.25:\n",
    "                print(\"cont\")\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            starting_point = 600000\n",
    "            plus_one = True     \n",
    "           \n",
    "            checkpoint = 'ema-'+str(starting_point)+'.tar'\n",
    "        \n",
    "            while checkpoint not in os.listdir(complete_path):\n",
    "                \n",
    "                if plus_one:\n",
    "                    starting_point -= 9999\n",
    "                    plus_one = False\n",
    "                else:\n",
    "                    starting_point -= 10000\n",
    "                checkpoint = 'ema-'+str(starting_point)+'.tar'\n",
    "                if starting_point < 50000:\n",
    "                    break\n",
    "            if starting_point < 50000:\n",
    "                    break\n",
    "                    \n",
    "            save_path = Path(\"./sampled_images/{}/{}_{}/\".format(dataset, dim_mults,aug_factor))\n",
    "            save_path.mkdir(exist_ok = True)\n",
    "            print(\"start_iteration\")\n",
    "            print(starting_point)\n",
    "            for m in range(5):\n",
    "                #Create the directory for the final images\n",
    "                checkpoint = 'ema-'+str(starting_point)+'.tar'\n",
    "                save_path = Path(\"./sampled_images/{}/{}_{}/{}/\".format(dataset,dim_mults,aug_factor,starting_point))\n",
    "                save_path.mkdir(exist_ok = True)\n",
    "                \n",
    "                \n",
    "                print(\"Checkpoint:\\t\",checkpoint)\n",
    "                result_list = [dataset, dim_mults, aug_factor]\n",
    "                #Delete all files in the sample directory\n",
    "                for f in os.listdir(sample_path):\n",
    "                    os.remove(os.path.join(sample_path, f))\n",
    "                    \n",
    "                    \n",
    "                model = Unet2(\n",
    "                    dim=64,\n",
    "                    channels=3,\n",
    "                    dim_mults=dim_mults,\n",
    "                    convnext_mult = conv_mult\n",
    "                    )\n",
    "                    \n",
    "                ema = EMA(\n",
    "                        model\n",
    "                    ).to(device)\n",
    "                \n",
    "                    \n",
    "                if checkpoint not in os.listdir(complete_path+'/'):\n",
    "                    if plus_one:\n",
    "                        starting_point -= 9999\n",
    "                        plus_one=False \n",
    "                    else:\n",
    "                        starting_point -= 10000\n",
    "                    continue\n",
    "\n",
    "                ema.load_state_dict(torch.load(complete_path+'/'+checkpoint, map_location=torch.device('cuda')))\n",
    "\n",
    "                ema.eval()\n",
    "                    \n",
    "                #Sample new images and normalize\n",
    "                all_images_list_ema = []\n",
    "                with torch.no_grad():\n",
    "                    for i in range(2):\n",
    "                        all_images_list_ema.append(ddim_sample(model, (16,3,96,96), device='cuda', total_timesteps=1000, sampling_timesteps=50, ddim_sampling_eta=0))\n",
    "\n",
    "                sample_image_ema = map_interval(torch.cat(all_images_list_ema, axis=0))\n",
    "                    \n",
    "                for j, image in enumerate(sample_image_ema):\n",
    "                    save_image((image+1)*0.5, './score_calculations/sample_{}.png'.format(j))\n",
    "                    \n",
    "                #Calculate LPIPS scores and closes sample\n",
    "                lpips_min_avg, lpips_min_min, lpips_fraction, overfit, closest_tensor = calculate_lpips(full_tens, sample_image_ema,intrinsic_df[intrinsic_df.Name==dataset]['LPIPS'].values)\n",
    "                    \n",
    "                #Calculate FID scores\n",
    "                print(dataset_path+\"/img/\")\n",
    "                fid_score = fid.compute_fid(dataset_path+\"/img/\", sample_path,num_workers=0)\n",
    "                \n",
    "\n",
    "                    \n",
    "                #Calculate KID scores\n",
    "                kid_score = fid.compute_kid(dataset_path, sample_path, mode='clean',num_workers=0)\n",
    "                \n",
    "                #Calculate Clip-FID scores\n",
    "                clip_fid = calculate_clip_FID(dataset)\n",
    "                print(clip_fid)\n",
    "                print(clip_fid[-1].replace(',','.'))\n",
    "                try:\n",
    "                    clip_fid = float(clip_fid[-1])\n",
    "                except:\n",
    "                    clip_fid = -1.  \n",
    "                    \n",
    "\n",
    "                save_image((sample_image_ema[0:32]+1)*0.5, \"./sampled_images/{}/{}_{}/{}/sample_images.png\".format(dataset,dim_mults,aug_factor,starting_point))\n",
    "                save_image((closest_tensor[0:32]+1)*0.5, \"./sampled_images/{}/{}_{}/{}/real_images.png\".format(dataset,dim_mults,aug_factor,starting_point))\n",
    "                for k in range(10):\n",
    "                    save_image((sample_image_ema[k]+1)*0.5, \"./sampled_images/{}/{}_{}/{}/sample_image_{}.png\".format(dataset,dim_mults,aug_factor,starting_point,k))\n",
    "                    save_image((closest_tensor[k]+1)*0.5, \"./sampled_images/{}/{}_{}/{}/real_image_{}.png\".format(dataset,dim_mults,aug_factor,starting_point,k))\n",
    "                    \n",
    "                    \n",
    "                #Write results into a result_list\n",
    "                result_list += [starting_point, fid_score, kid_score, clip_fid, lpips_min_avg, lpips_min_min, lpips_fraction, overfit]\n",
    "                print(result_list)\n",
    "                if plus_one:\n",
    "                    starting_point -= 19999\n",
    "                    plus_one=False \n",
    "                elif not plus_one and overfit:\n",
    "                    starting_point -= 40000\n",
    "                else:\n",
    "                    starting_point -= 20000\n",
    "                    \n",
    "                result_dict[dataset+str(dim_mults)+str(m)+'_'+str(aug_factor)] = result_list\n",
    "        \n",
    "            \n",
    "                result_df= pd.DataFrame.from_dict(result_dict,orient='index').transpose().set_index('Features').transpose()\n",
    "                display(result_df)\n",
    "                result_df.to_csv('./score_tables/all_scores.csv')\n",
    "                        \n",
    "                    \n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fe967654",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\floru\\AppData\\Local\\Temp\\ipykernel_20356\\3537924115.py:151: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  intrinsic_df.loc[:,'Name'][intrinsic_df.Name=='Shell'] = 'Shells'\n",
      "C:\\Users\\floru\\AppData\\Local\\Temp\\ipykernel_20356\\3537924115.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  intrinsic_df.loc[:,'Name'][intrinsic_df.Name=='Skull'] = 'Skulls'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>LPIPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Obama</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grumpy</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cat</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dog</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Panda</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Influ</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Moon</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Anime</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Poke</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Art</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fauvism</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Flat</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Shells</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Skulls</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FFHQ</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LSUNBed</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CelebA</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Flowers</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cars</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Garbage</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Dining</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name  LPIPS\n",
       "0     Obama   0.21\n",
       "1    Grumpy   0.20\n",
       "2       Cat   0.21\n",
       "3       Dog   0.20\n",
       "4     Panda   0.22\n",
       "5     Influ   0.27\n",
       "6      Moon   0.25\n",
       "7     Anime   0.23\n",
       "8      Poke   0.24\n",
       "9       Art   0.26\n",
       "10  Fauvism   0.27\n",
       "11     Flat   0.26\n",
       "12   Shells   0.21\n",
       "13   Skulls   0.22\n",
       "14     FFHQ   0.20\n",
       "15  LSUNBed   0.28\n",
       "16   CelebA   0.20\n",
       "17  Flowers   0.26\n",
       "18     Cars   0.26\n",
       "19  Garbage   0.26\n",
       "20   Dining   0.26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 96, 96])\n",
      "run:  CelebA96_linear_16_64_(1, 2)_1000_0.25_l20.0003_1\n",
      "start_iteration\n",
      "600000\n",
      "Checkpoint:\t ema-600000.tar\n",
      "init_dim:\t None\n",
      "dims:\t\t [42, 64, 128]\n",
      "in_out:\t\t [(42, 64), (64, 128)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ff8cab03504edc9be6cdcddddfded3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8051daaa854c7a8ceff3f7f4c98585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./train_data/CelebA/img/\n",
      "./\n",
      "./inception-2015-12-05.pt\n",
      "<torch.jit.CompilationUnit object at 0x0000029376D39A30>\n",
      "None\n",
      "{}\n",
      "compute FID between two folders\n",
      "Found 200 images in the folder ./train_data/CelebA/img/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID  : 100%|█████████████████████████████████████████████████████████████████████████████| 7/7 [00:08<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 64 images in the folder ./score_calculations/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID  : 100%|█████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./inception-2015-12-05.pt\n",
      "<torch.jit.CompilationUnit object at 0x0000029376D77630>\n",
      "None\n",
      "{}\n",
      "compute KID between two folders\n",
      "Found 200 images in the folder ./train_data/CelebA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KID CelebA : 100%|███████████████████████████████████████████████████████████████████████| 7/7 [00:08<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 64 images in the folder ./score_calculations/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KID  : 100%|█████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CelebA\n",
      "['Preprocess Images from : ./train_data/CelebA/img/', '', '  0%|          | 0/100 [00:00<?, ?it/s]', ' 55%|#####5    | 55/100 [00:00<00:00, 547.11it/s]', '100%|##########| 100/100 [00:00<00:00, 579.46it/s]', 'Preprocess Images from : ./score_calculations/', '', '  0%|          | 0/32 [00:00<?, ?it/s]', '100%|##########| 32/32 [00:00<00:00, 695.65it/s]', 'Infernce from CLIP', '', '', '0it [00:00, ?it/s]', '1it [00:03,  3.03s/it]', '1it [00:03,  3.03s/it]', 'Calc FCD Score:', '', '47.41742767055652']\n",
      "47.41742767055652\n",
      "['CelebA', (1, 2), 0.25, 600000, 150.0366949594007, 0.042815012137095175, 47.41742767055652, 0.16459925496019423, 0.10233676806092262, 0.8229962748009712, False]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Features</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Dim_mults</th>\n",
       "      <th>Augmentation</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>FID</th>\n",
       "      <th>KID</th>\n",
       "      <th>Clip_FID</th>\n",
       "      <th>LPIPS_MinAvg</th>\n",
       "      <th>LPIPS_min</th>\n",
       "      <th>LPIPS_norm</th>\n",
       "      <th>Overfit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CelebA(1, 2)0_0.25</th>\n",
       "      <td>CelebA</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.25</td>\n",
       "      <td>600000</td>\n",
       "      <td>150.036695</td>\n",
       "      <td>0.042815</td>\n",
       "      <td>47.417428</td>\n",
       "      <td>0.164599</td>\n",
       "      <td>0.102337</td>\n",
       "      <td>0.822996</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Features           Dataset Dim_mults Augmentation Iteration         FID  \\\n",
       "CelebA(1, 2)0_0.25  CelebA    (1, 2)         0.25    600000  150.036695   \n",
       "\n",
       "Features                 KID   Clip_FID LPIPS_MinAvg LPIPS_min LPIPS_norm  \\\n",
       "CelebA(1, 2)0_0.25  0.042815  47.417428     0.164599  0.102337   0.822996   \n",
       "\n",
       "Features           Overfit  \n",
       "CelebA(1, 2)0_0.25   False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint:\t ema-580001.tar\n",
      "init_dim:\t None\n",
      "dims:\t\t [42, 64, 128]\n",
      "in_out:\t\t [(42, 64), (64, 128)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca16080f977493089cf3e640453df80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0737fd60304374964f1dbb29d700a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./train_data/CelebA/img/\n",
      "./\n",
      "./inception-2015-12-05.pt\n",
      "<torch.jit.CompilationUnit object at 0x0000029374600C70>\n",
      "None\n",
      "{}\n",
      "compute FID between two folders\n",
      "Found 200 images in the folder ./train_data/CelebA/img/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID  : 100%|█████████████████████████████████████████████████████████████████████████████| 7/7 [00:08<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 64 images in the folder ./score_calculations/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID  : 100%|█████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./inception-2015-12-05.pt\n",
      "<torch.jit.CompilationUnit object at 0x00000293744C3F70>\n",
      "None\n",
      "{}\n",
      "compute KID between two folders\n",
      "Found 200 images in the folder ./train_data/CelebA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KID CelebA : 100%|███████████████████████████████████████████████████████████████████████| 7/7 [00:08<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 64 images in the folder ./score_calculations/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KID  : 100%|█████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CelebA\n",
      "['Preprocess Images from : ./train_data/CelebA/img/', '', '  0%|          | 0/100 [00:00<?, ?it/s]', ' 70%|#######   | 70/100 [00:00<00:00, 689.57it/s]', '100%|##########| 100/100 [00:00<00:00, 709.11it/s]', 'Preprocess Images from : ./score_calculations/', '', '  0%|          | 0/32 [00:00<?, ?it/s]', '100%|##########| 32/32 [00:00<00:00, 727.20it/s]', 'Infernce from CLIP', '', '', '0it [00:00, ?it/s]', '1it [00:04,  4.14s/it]', '1it [00:04,  4.14s/it]', 'Calc FCD Score:', '', '49.94611490685234']\n",
      "49.94611490685234\n",
      "['CelebA', (1, 2), 0.25, 580001, 153.57381260982584, 0.05216074379663618, 49.94611490685234, 0.1812200634740293, 0.11702277511358261, 0.9061003173701465, False]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Features</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Dim_mults</th>\n",
       "      <th>Augmentation</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>FID</th>\n",
       "      <th>KID</th>\n",
       "      <th>Clip_FID</th>\n",
       "      <th>LPIPS_MinAvg</th>\n",
       "      <th>LPIPS_min</th>\n",
       "      <th>LPIPS_norm</th>\n",
       "      <th>Overfit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CelebA(1, 2)0_0.25</th>\n",
       "      <td>CelebA</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.25</td>\n",
       "      <td>600000</td>\n",
       "      <td>150.036695</td>\n",
       "      <td>0.042815</td>\n",
       "      <td>47.417428</td>\n",
       "      <td>0.164599</td>\n",
       "      <td>0.102337</td>\n",
       "      <td>0.822996</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CelebA(1, 2)1_0.25</th>\n",
       "      <td>CelebA</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.25</td>\n",
       "      <td>580001</td>\n",
       "      <td>153.573813</td>\n",
       "      <td>0.052161</td>\n",
       "      <td>49.946115</td>\n",
       "      <td>0.18122</td>\n",
       "      <td>0.117023</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Features           Dataset Dim_mults Augmentation Iteration         FID  \\\n",
       "CelebA(1, 2)0_0.25  CelebA    (1, 2)         0.25    600000  150.036695   \n",
       "CelebA(1, 2)1_0.25  CelebA    (1, 2)         0.25    580001  153.573813   \n",
       "\n",
       "Features                 KID   Clip_FID LPIPS_MinAvg LPIPS_min LPIPS_norm  \\\n",
       "CelebA(1, 2)0_0.25  0.042815  47.417428     0.164599  0.102337   0.822996   \n",
       "CelebA(1, 2)1_0.25  0.052161  49.946115      0.18122  0.117023     0.9061   \n",
       "\n",
       "Features           Overfit  \n",
       "CelebA(1, 2)0_0.25   False  \n",
       "CelebA(1, 2)1_0.25   False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint:\t ema-560001.tar\n",
      "init_dim:\t None\n",
      "dims:\t\t [42, 64, 128]\n",
      "in_out:\t\t [(42, 64), (64, 128)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5bd78ee66f45a9899aec7af7067b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fcf0c78ce73431b8e66bd22b1458373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./train_data/CelebA/img/\n",
      "./\n",
      "./inception-2015-12-05.pt\n",
      "<torch.jit.CompilationUnit object at 0x0000029374CECBF0>\n",
      "None\n",
      "{}\n",
      "compute FID between two folders\n",
      "Found 200 images in the folder ./train_data/CelebA/img/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID  : 100%|█████████████████████████████████████████████████████████████████████████████| 7/7 [00:08<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 64 images in the folder ./score_calculations/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID  : 100%|█████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./inception-2015-12-05.pt\n",
      "<torch.jit.CompilationUnit object at 0x0000029376D4C7B0>\n",
      "None\n",
      "{}\n",
      "compute KID between two folders\n",
      "Found 200 images in the folder ./train_data/CelebA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KID CelebA : 100%|███████████████████████████████████████████████████████████████████████| 7/7 [00:08<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 64 images in the folder ./score_calculations/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KID  : 100%|█████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CelebA\n",
      "['Preprocess Images from : ./train_data/CelebA/img/', '', '  0%|          | 0/100 [00:00<?, ?it/s]', ' 70%|#######   | 70/100 [00:00<00:00, 696.43it/s]', '100%|##########| 100/100 [00:00<00:00, 709.11it/s]', 'Preprocess Images from : ./score_calculations/', '', '  0%|          | 0/32 [00:00<?, ?it/s]', '100%|##########| 32/32 [00:00<00:00, 711.08it/s]', 'Infernce from CLIP', '', '', '0it [00:00, ?it/s]', '1it [00:04,  4.17s/it]', '1it [00:04,  4.17s/it]', 'Calc FCD Score:', '', '48.89877148784375']\n",
      "48.89877148784375\n",
      "['CelebA', (1, 2), 0.25, 560001, 142.77305663633018, 0.038254205612909244, 48.89877148784375, 0.17621236538980156, 0.08999713696539402, 0.8810618269490078, False]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Features</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Dim_mults</th>\n",
       "      <th>Augmentation</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>FID</th>\n",
       "      <th>KID</th>\n",
       "      <th>Clip_FID</th>\n",
       "      <th>LPIPS_MinAvg</th>\n",
       "      <th>LPIPS_min</th>\n",
       "      <th>LPIPS_norm</th>\n",
       "      <th>Overfit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CelebA(1, 2)0_0.25</th>\n",
       "      <td>CelebA</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.25</td>\n",
       "      <td>600000</td>\n",
       "      <td>150.036695</td>\n",
       "      <td>0.042815</td>\n",
       "      <td>47.417428</td>\n",
       "      <td>0.164599</td>\n",
       "      <td>0.102337</td>\n",
       "      <td>0.822996</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CelebA(1, 2)1_0.25</th>\n",
       "      <td>CelebA</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.25</td>\n",
       "      <td>580001</td>\n",
       "      <td>153.573813</td>\n",
       "      <td>0.052161</td>\n",
       "      <td>49.946115</td>\n",
       "      <td>0.18122</td>\n",
       "      <td>0.117023</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CelebA(1, 2)2_0.25</th>\n",
       "      <td>CelebA</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.25</td>\n",
       "      <td>560001</td>\n",
       "      <td>142.773057</td>\n",
       "      <td>0.038254</td>\n",
       "      <td>48.898771</td>\n",
       "      <td>0.176212</td>\n",
       "      <td>0.089997</td>\n",
       "      <td>0.881062</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Features           Dataset Dim_mults Augmentation Iteration         FID  \\\n",
       "CelebA(1, 2)0_0.25  CelebA    (1, 2)         0.25    600000  150.036695   \n",
       "CelebA(1, 2)1_0.25  CelebA    (1, 2)         0.25    580001  153.573813   \n",
       "CelebA(1, 2)2_0.25  CelebA    (1, 2)         0.25    560001  142.773057   \n",
       "\n",
       "Features                 KID   Clip_FID LPIPS_MinAvg LPIPS_min LPIPS_norm  \\\n",
       "CelebA(1, 2)0_0.25  0.042815  47.417428     0.164599  0.102337   0.822996   \n",
       "CelebA(1, 2)1_0.25  0.052161  49.946115      0.18122  0.117023     0.9061   \n",
       "CelebA(1, 2)2_0.25  0.038254  48.898771     0.176212  0.089997   0.881062   \n",
       "\n",
       "Features           Overfit  \n",
       "CelebA(1, 2)0_0.25   False  \n",
       "CelebA(1, 2)1_0.25   False  \n",
       "CelebA(1, 2)2_0.25   False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint:\t ema-540001.tar\n",
      "init_dim:\t None\n",
      "dims:\t\t [42, 64, 128]\n",
      "in_out:\t\t [(42, 64), (64, 128)]\n",
      "Checkpoint:\t ema-530001.tar\n",
      "init_dim:\t None\n",
      "dims:\t\t [42, 64, 128]\n",
      "in_out:\t\t [(42, 64), (64, 128)]\n"
     ]
    }
   ],
   "source": [
    "df = test_run_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13a151b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterthesis_kernel",
   "language": "python",
   "name": "masterthesis_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
